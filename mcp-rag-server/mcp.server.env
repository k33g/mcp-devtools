#MODEL_RUNNER_BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1
MODEL_RUNNER_BASE_URL=http://localhost:12434/engines/llama.cpp/v1
EMBEDDING_MODEL=ai/granite-embedding-multilingual:latest
MCP_HTTP_PORT=9095
LIMIT=0.45
MAX_RESULTS=30
JSON_STORE_FILE_PATH=store/rag-memory-store.json
CHUNK_SIZE=1024
CHUNK_OVERLAP=512
